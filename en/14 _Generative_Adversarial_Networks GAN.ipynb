{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN4k59mu0Q6iHyxOWvnjE+m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TQuq7QfFu7p","executionInfo":{"status":"ok","timestamp":1763547053825,"user_tz":-60,"elapsed":1221435,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"448e3d05-a023-4b5e-afb8-046f9d45c4ee"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 10.4MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 344kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 3.16MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 8.86MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training on [cuda]...\n","Epoch [1/100] - D_loss: 1.1749, G_loss: 1.6221\n","Epoch [2/100] - D_loss: 1.2778, G_loss: 1.0727\n","Epoch [3/100] - D_loss: 0.8129, G_loss: 1.6295\n","Epoch [4/100] - D_loss: 1.0287, G_loss: 1.4112\n","Epoch [5/100] - D_loss: 1.0626, G_loss: 1.3379\n","Epoch [6/100] - D_loss: 1.0472, G_loss: 1.3881\n","Epoch [7/100] - D_loss: 1.1335, G_loss: 1.1917\n","Epoch [8/100] - D_loss: 1.1721, G_loss: 1.1215\n","Epoch [9/100] - D_loss: 1.1580, G_loss: 1.1515\n","Epoch [10/100] - D_loss: 1.1929, G_loss: 1.0591\n","Epoch [11/100] - D_loss: 1.2373, G_loss: 0.9806\n","Epoch [12/100] - D_loss: 1.2512, G_loss: 0.9451\n","Epoch [13/100] - D_loss: 1.2479, G_loss: 0.9647\n","Epoch [14/100] - D_loss: 1.2114, G_loss: 1.0630\n","Epoch [15/100] - D_loss: 1.1899, G_loss: 1.0645\n","Epoch [16/100] - D_loss: 1.2174, G_loss: 0.9992\n","Epoch [17/100] - D_loss: 1.2410, G_loss: 0.9780\n","Epoch [18/100] - D_loss: 1.2201, G_loss: 1.0007\n","Epoch [19/100] - D_loss: 1.2331, G_loss: 0.9924\n","Epoch [20/100] - D_loss: 1.2278, G_loss: 1.0020\n","Epoch [21/100] - D_loss: 1.2417, G_loss: 0.9617\n","Epoch [22/100] - D_loss: 1.2404, G_loss: 0.9661\n","Epoch [23/100] - D_loss: 1.2443, G_loss: 0.9790\n","Epoch [24/100] - D_loss: 1.2447, G_loss: 0.9543\n","Epoch [25/100] - D_loss: 1.2498, G_loss: 0.9698\n","Epoch [26/100] - D_loss: 1.2389, G_loss: 0.9739\n","Epoch [27/100] - D_loss: 1.2438, G_loss: 0.9785\n","Epoch [28/100] - D_loss: 1.2495, G_loss: 0.9589\n","Epoch [29/100] - D_loss: 1.2679, G_loss: 0.9179\n","Epoch [30/100] - D_loss: 1.2657, G_loss: 0.9409\n","Epoch [31/100] - D_loss: 1.2544, G_loss: 0.9499\n","Epoch [32/100] - D_loss: 1.2597, G_loss: 0.9568\n","Epoch [33/100] - D_loss: 1.2426, G_loss: 0.9778\n","Epoch [34/100] - D_loss: 1.2596, G_loss: 0.9565\n","Epoch [35/100] - D_loss: 1.2536, G_loss: 0.9384\n","Epoch [36/100] - D_loss: 1.2636, G_loss: 0.9678\n","Epoch [37/100] - D_loss: 1.2229, G_loss: 1.0122\n","Epoch [38/100] - D_loss: 1.2484, G_loss: 0.9536\n","Epoch [39/100] - D_loss: 1.2573, G_loss: 0.9365\n","Epoch [40/100] - D_loss: 1.2566, G_loss: 0.9569\n","Epoch [41/100] - D_loss: 1.2559, G_loss: 0.9486\n","Epoch [42/100] - D_loss: 1.2457, G_loss: 0.9669\n","Epoch [43/100] - D_loss: 1.2568, G_loss: 0.9502\n","Epoch [44/100] - D_loss: 1.2601, G_loss: 0.9593\n","Epoch [45/100] - D_loss: 1.2551, G_loss: 0.9712\n","Epoch [46/100] - D_loss: 1.2615, G_loss: 0.9200\n","Epoch [47/100] - D_loss: 1.2532, G_loss: 0.9682\n","Epoch [48/100] - D_loss: 1.2453, G_loss: 0.9685\n","Epoch [49/100] - D_loss: 1.2631, G_loss: 0.9321\n","Epoch [50/100] - D_loss: 1.2495, G_loss: 0.9444\n","Epoch [51/100] - D_loss: 1.2459, G_loss: 0.9760\n","Epoch [52/100] - D_loss: 1.2377, G_loss: 0.9719\n","Epoch [53/100] - D_loss: 1.2496, G_loss: 0.9540\n","Epoch [54/100] - D_loss: 1.2438, G_loss: 0.9768\n","Epoch [55/100] - D_loss: 1.2422, G_loss: 0.9887\n","Epoch [56/100] - D_loss: 1.2490, G_loss: 0.9447\n","Epoch [57/100] - D_loss: 1.2539, G_loss: 0.9443\n","Epoch [58/100] - D_loss: 1.2536, G_loss: 0.9431\n","Epoch [59/100] - D_loss: 1.2519, G_loss: 0.9547\n","Epoch [60/100] - D_loss: 1.2495, G_loss: 0.9417\n","Epoch [61/100] - D_loss: 1.2457, G_loss: 0.9487\n","Epoch [62/100] - D_loss: 1.2472, G_loss: 0.9648\n","Epoch [63/100] - D_loss: 1.2470, G_loss: 0.9544\n","Epoch [64/100] - D_loss: 1.2474, G_loss: 0.9559\n","Epoch [65/100] - D_loss: 1.2501, G_loss: 0.9597\n","Epoch [66/100] - D_loss: 1.2453, G_loss: 0.9577\n","Epoch [67/100] - D_loss: 1.2450, G_loss: 0.9619\n","Epoch [68/100] - D_loss: 1.2339, G_loss: 0.9890\n","Epoch [69/100] - D_loss: 1.2391, G_loss: 0.9618\n","Epoch [70/100] - D_loss: 1.2321, G_loss: 0.9844\n","Epoch [71/100] - D_loss: 1.2299, G_loss: 0.9916\n","Epoch [72/100] - D_loss: 1.2379, G_loss: 0.9722\n","Epoch [73/100] - D_loss: 1.2436, G_loss: 0.9635\n","Epoch [74/100] - D_loss: 1.2309, G_loss: 0.9875\n","Epoch [75/100] - D_loss: 1.2376, G_loss: 0.9899\n","Epoch [76/100] - D_loss: 1.2211, G_loss: 0.9969\n","Epoch [77/100] - D_loss: 1.2239, G_loss: 0.9815\n","Epoch [78/100] - D_loss: 1.2331, G_loss: 0.9923\n","Epoch [79/100] - D_loss: 1.2254, G_loss: 0.9932\n","Epoch [80/100] - D_loss: 1.2172, G_loss: 1.0107\n","Epoch [81/100] - D_loss: 1.2273, G_loss: 0.9932\n","Epoch [82/100] - D_loss: 1.2326, G_loss: 0.9834\n","Epoch [83/100] - D_loss: 1.2302, G_loss: 0.9783\n","Epoch [84/100] - D_loss: 1.2193, G_loss: 1.0203\n","Epoch [85/100] - D_loss: 1.2188, G_loss: 0.9997\n","Epoch [86/100] - D_loss: 1.2212, G_loss: 1.0154\n","Epoch [87/100] - D_loss: 1.2147, G_loss: 1.0106\n","Epoch [88/100] - D_loss: 1.2178, G_loss: 1.0109\n","Epoch [89/100] - D_loss: 1.2139, G_loss: 1.0102\n","Epoch [90/100] - D_loss: 1.2232, G_loss: 1.0036\n","Epoch [91/100] - D_loss: 1.2262, G_loss: 0.9930\n","Epoch [92/100] - D_loss: 1.2233, G_loss: 1.0129\n","Epoch [93/100] - D_loss: 1.2138, G_loss: 1.0170\n","Epoch [94/100] - D_loss: 1.2075, G_loss: 1.0134\n","Epoch [95/100] - D_loss: 1.2026, G_loss: 1.0330\n","Epoch [96/100] - D_loss: 1.2014, G_loss: 1.0253\n","Epoch [97/100] - D_loss: 1.1971, G_loss: 1.0308\n","Epoch [98/100] - D_loss: 1.1977, G_loss: 1.0363\n","Epoch [99/100] - D_loss: 1.2014, G_loss: 1.0332\n","Epoch [100/100] - D_loss: 1.2042, G_loss: 1.0260\n"]}],"source":["import math\n","import pickle as pkl\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# Load data\n","transform = transforms.Compose([transforms.ToTensor()])\n","train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","dl = DataLoader(dataset=train_ds, shuffle=True, batch_size=64)\n","\n","# Architectures\n","class Discriminator(nn.Module):\n","    def __init__(self, in_features=784, out_features=1):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 32),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(32, out_features)\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","        return self.model(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_features=100, out_features=784):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features, 32),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(32, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, out_features),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Loss functions\n","def compute_loss(logits, target_value, loss_fn, device):\n","    \"\"\"Computes loss for real outputs (target=1) or fake outputs (target=0)\"\"\"\n","    batch_size = logits.shape[0]\n","    targets = torch.full((batch_size,), target_value, device=device)\n","    return loss_fn(logits.squeeze(), targets)\n","\n","# Training\n","def train_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device, z_size=100):\n","    print(f'Training on [{device}]...')\n","\n","    # Fixed latent vector for monitoring progress\n","    fixed_z = torch.randn(16, z_size, device=device)\n","    fixed_samples = []\n","    d_losses, g_losses = [], []\n","\n","    d.to(device)\n","    g.to(device)\n","\n","    for epoch in range(n_epochs):\n","        d.train()\n","        g.train()\n","        d_running_loss = 0\n","        g_running_loss = 0\n","\n","        for real_images, _ in dl:\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Train Discriminator\n","            d_optim.zero_grad()\n","\n","            # With real images (scaled to [-1, 1])\n","            real_images = (real_images * 2) - 1\n","            d_real_out = d(real_images)\n","            d_real_loss = compute_loss(d_real_out, 1.0, loss_fn, device)\n","\n","            # With fake images\n","            z = torch.randn(batch_size, z_size, device=device)\n","            fake_images = g(z).detach()\n","            d_fake_out = d(fake_images)\n","            d_fake_loss = compute_loss(d_fake_out, 0.0, loss_fn, device)\n","\n","            d_loss = d_real_loss + d_fake_loss\n","            d_loss.backward()\n","            d_optim.step()\n","            d_running_loss += d_loss.item()\n","\n","            # Train Generator\n","            g_optim.zero_grad()\n","\n","            z = torch.randn(batch_size, z_size, device=device)\n","            fake_images = g(z)\n","            g_out = d(fake_images)\n","            g_loss = compute_loss(g_out, 1.0, loss_fn, device)\n","\n","            g_loss.backward()\n","            g_optim.step()\n","            g_running_loss += g_loss.item()\n","\n","        # Save epoch losses\n","        d_losses.append(d_running_loss / len(dl))\n","        g_losses.append(g_running_loss / len(dl))\n","\n","        print(f'Epoch [{epoch+1}/{n_epochs}] - D_loss: {d_losses[-1]:.4f}, G_loss: {g_losses[-1]:.4f}')\n","\n","        # Save generator samples\n","        g.eval()\n","        with torch.no_grad():\n","            fixed_samples.append(g(fixed_z).cpu())\n","\n","    # Save fixed samples\n","    with open('fixed_samples.pkl', 'wb') as f:\n","        pkl.dump(fixed_samples, f)\n","\n","    return d_losses, g_losses\n","\n","# Setup and training\n","d = Discriminator()\n","g = Generator()\n","\n","d_optim = optim.Adam(d.parameters(), lr=0.002)\n","g_optim = optim.Adam(g.parameters(), lr=0.002)\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Train\n","n_epochs = 100\n","d_losses, g_losses = train_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device)\n"]}]}