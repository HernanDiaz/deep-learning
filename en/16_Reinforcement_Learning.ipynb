{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgOUfZbs+oWDALvJP7N4Zk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybJmzbKz7V6X","executionInfo":{"status":"ok","timestamp":1763544832385,"user_tz":-60,"elapsed":75,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"87d057dc-169e-4787-d5dc-31d6be05f4e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final estimated Q-values: [0.17647059 0.79977629 0.375      0.4        0.90789474]\n","True values: [0.2, 0.8, 0.5, 0.3, 0.9]\n","Average reward: 0.792\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Class that represents the multi-armed bandit environment\n","class BanditEnvironment:\n","   def __init__(self):\n","       # Initialization of the true reward probabilities for each arm\n","       self.true_rewards = [0.2, 0.8, 0.5, 0.3, 0.9]\n","\n","   # Method that simulates executing an action (pulling an arm)\n","   def step(self, action):\n","       \"\"\"Executes action and returns reward\"\"\"\n","       # Generates a binary reward (1 or 0) based on the probability of the selected arm\n","       reward = 1 if np.random.random() < self.true_rewards[action] else 0\n","       return reward\n","\n","# Class that implements an epsilon-greedy agent\n","class EpsilonGreedyAgent:\n","   def __init__(self, n_actions, epsilon=0.1, decay_rate=0.99):\n","       # Number of possible actions (bandit arms)\n","       self.n_actions = n_actions\n","       # Initial exploration probability\n","       self.epsilon = epsilon\n","       # Decay rate for epsilon\n","       self.decay_rate = decay_rate\n","       # Estimated Q-values for each action (initialized to 0)\n","       self.q_values = np.zeros(n_actions)\n","       # Counter for how many times each action has been selected\n","       self.action_counts = np.zeros(n_actions)\n","\n","   # Method to select an action following the epsilon-greedy strategy\n","   def select_action(self):\n","       # With probability epsilon, select a random action (exploration)\n","       if np.random.random() < self.epsilon:\n","           return np.random.randint(self.n_actions)\n","       # With probability 1 - epsilon, select the best known action (exploitation)\n","       else:\n","           return np.argmax(self.q_values)\n","\n","   # Method to update the Q-value estimate of an action\n","   def update_q_value(self, action, reward):\n","       # Increment the counter for the selected action\n","       self.action_counts[action] += 1\n","       # Update the Q-value using incremental averaging\n","       self.q_values[action] += (reward - self.q_values[action]) / self.action_counts[action]\n","\n","   # Method to gradually reduce epsilon (less exploration over time)\n","   def decay_epsilon(self):\n","       self.epsilon *= self.decay_rate\n","\n","# Function that simulates the bandit problem with the epsilon-greedy agent\n","def simulate_bandit_problem():\n","   # Create the environment with the true rewards\n","   env = BanditEnvironment()\n","   # Create the agent with 5 actions and initial epsilon = 0.3\n","   agent = EpsilonGreedyAgent(n_actions=5, epsilon=0.3)\n","\n","   # Lists to store reward and action history\n","   rewards_history = []\n","   actions_history = []\n","\n","   # Main simulation loop (1000 steps)\n","   for step in range(1000):\n","       # 1. The agent selects an action\n","       action = agent.select_action()\n","\n","       # 2. The environment returns a reward for that action\n","       reward = env.step(action)\n","\n","       # 3. The agent updates its estimates with the received reward\n","       agent.update_q_value(action, reward)\n","       # Gradually reduce epsilon\n","       agent.decay_epsilon()\n","\n","       # Store history for further analysis\n","       rewards_history.append(reward)\n","       actions_history.append(action)\n","\n","   return rewards_history, actions_history, agent.q_values\n","\n","# Run the simulation and obtain results\n","rewards, actions, final_q_values = simulate_bandit_problem()\n","\n","# Print final results\n","print(\"Final estimated Q-values:\", final_q_values)\n","print(\"True values:\", [0.2, 0.8, 0.5, 0.3, 0.9])\n","print(\"Average reward:\", np.mean(rewards))\n"]}]}