{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlYFS+qKgjj71w9scI13sa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0XlszNFWpI3","executionInfo":{"status":"ok","timestamp":1763545572503,"user_tz":-60,"elapsed":114,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"b8716ae0-5ea4-401a-a227-7c9565c21d3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Policy learned by Q-Learning:\n","> > v v \n","v > > v \n","v v v v \n","> > > ^ \n","--------------------\n","Maximum Q-values per cell:\n"," 1.81  3.12  4.58  6.20 \n"," 3.12  4.58  6.20  8.00 \n"," 4.58  5.16  8.00 10.00 \n"," 6.20  8.00 10.00  0.00 \n","--------------------\n"]}],"source":["import numpy as np\n","\n","class GridWorldQLearning:\n","    def __init__(self, size=4, obstacles=None, start=(0,0), goal=(3,3)):\n","        self.size = size\n","        self.n_states = size * size\n","        self.n_actions = 4  # up, down, left, right\n","\n","        self.start_state = start\n","        self.goal_state = goal\n","        self.obstacles = obstacles if obstacles else [(1,1),(2,2)]\n","\n","        # Actions\n","        self.actions = {\n","            0: (-1, 0),  # up\n","            1: (1, 0),   # down\n","            2: (0, -1),  # left\n","            3: (0, 1)    # right\n","        }\n","\n","    def _pos_to_state(self, pos):\n","        return pos[0] * self.size + pos[1]\n","\n","    def _state_to_pos(self, state):\n","        return (state // self.size, state % self.size)\n","\n","    def _is_valid_position(self, pos):\n","        return 0 <= pos[0] < self.size and 0 <= pos[1] < self.size\n","\n","    def step(self, state, action):\n","        \"\"\"Executes the action and returns next_state, reward, done\"\"\"\n","        pos = self._state_to_pos(state)\n","        move = self.actions[action]\n","        new_pos = (pos[0] + move[0], pos[1] + move[1])\n","\n","        if not self._is_valid_position(new_pos):\n","            new_pos = pos  # stay in the same place\n","\n","        reward = -1  # normal cost\n","        if new_pos in self.obstacles:\n","            reward = -5\n","        if new_pos == self.goal_state:\n","            reward = 10\n","\n","        done = new_pos == self.goal_state\n","        next_state = self._pos_to_state(new_pos)\n","        return next_state, reward, done\n","\n","    def q_learning(self, episodes=500, alpha=0.5, gamma=0.9, epsilon=0.1):\n","        Q = np.zeros((self.n_states, self.n_actions))\n","\n","        for ep in range(episodes):\n","            state = self._pos_to_state(self.start_state)\n","            done = False\n","\n","            while not done:\n","                # epsilon-greedy\n","                if np.random.rand() < epsilon:\n","                    action = np.random.randint(self.n_actions)\n","                else:\n","                    action = np.argmax(Q[state])\n","\n","                next_state, reward, done = self.step(state, action)\n","\n","                # Q-Learning update\n","                Q[state, action] = Q[state, action] + alpha * (\n","                    reward + gamma * np.max(Q[next_state]) - Q[state, action]\n","                )\n","\n","                state = next_state\n","\n","        # Extract final policy\n","        policy = np.argmax(Q, axis=1)\n","        return Q, policy\n","\n","    def print_policy_matrix(self, policy):\n","        arrow_symbols = ['^', 'v', '<', '>']\n","        for i in range(self.size):\n","            row_str = \"\"\n","            for j in range(self.size):\n","                state = self._pos_to_state((i,j))\n","                row_str += arrow_symbols[policy[state]] + \" \"\n","            print(row_str)\n","        print(\"-\"*20)\n","\n","    def print_q_max_matrix(self, Q):\n","        \"\"\"Prints the maximum Q(s,a) value of each cell in matrix format\"\"\"\n","        Q_max = np.max(Q, axis=1).reshape(self.size, self.size)\n","        for i in range(self.size):\n","            row_str = \"\"\n","            for j in range(self.size):\n","                row_str += f\"{Q_max[i,j]:5.2f} \"\n","            print(row_str)\n","        print(\"-\"*20)\n","\n","grid = GridWorldQLearning()\n","Q, policy = grid.q_learning(episodes=1000, alpha=0.5, gamma=0.9, epsilon=0.1)\n","\n","print(\"Policy learned by Q-Learning:\")\n","grid.print_policy_matrix(policy)\n","\n","print(\"Maximum Q-values per cell:\")\n","grid.print_q_max_matrix(Q)\n"]}]}